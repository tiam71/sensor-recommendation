import pandas as pd
import re
import torch
from sentence_transformers import SentenceTransformer, util
import numpy as np
import ast
from difflib import get_close_matches
from pydantic import BaseModel, Field

# åˆå§‹åŒ–
def initialize_system(csv_file="sensors.csv"):
    try:
        df = pd.read_csv(csv_file)
        print(f"è¼‰å…¥ {len(df)} ç­†æ„Ÿæ¸¬å™¨è³‡æ–™")
        
        # è™•ç† compatible_modules æ¬„ä½
        df["compatible_modules"] = df["compatible_modules"].fillna("")
        df["parsed_modules"] = df["compatible_modules"].apply(parse_compatible_modules)
        
        # å¢å¼·text
        df["search_text"] = df.apply(lambda row: create_enhanced_search_text(row), axis=1)

        print("è¼‰å…¥èªæ„æ¨¡å‹...")
        model = SentenceTransformer("./model")

        print("å»ºç«‹èªæ„å‘é‡...")
        device_embeddings = model.encode(df["search_text"].tolist(), convert_to_tensor=True)
        
        print("ç³»çµ±åˆå§‹åŒ–å®Œæˆï¼")
        return df, model, device_embeddings
        
    except Exception as e:
        print(f"åˆå§‹åŒ–éŒ¯èª¤ï¼š{e}")
        return None, None, None

def parse_compatible_modules(modules_str):
    if not modules_str or modules_str == "":
        return []
    
    try:
        cleaned = str(modules_str).strip('{}').replace('"', '').replace("'", "")
        if not cleaned:
            return []
        modules = [module.strip() for module in cleaned.split(',')]
        return [module for module in modules if module]
    except Exception as e:
        return []

def create_enhanced_search_text(row):
    text_parts = []
    
    # æ„Ÿæ¸¬å™¨åç¨±å’Œé¡å‹ (é«˜æ¬Šé‡)
    if pd.notna(row.get('name')):
        name = str(row['name'])
        text_parts.extend([name] * 5)
    
    if pd.notna(row.get('type')):
        sensor_type = str(row['type'])
        text_parts.extend([sensor_type] * 4)
    
    # ç›¸å®¹æ¨¡çµ„ (æœ€é«˜æ¬Šé‡)
    if 'parsed_modules' in row.index and row['parsed_modules']:
        for module in row['parsed_modules']:
            text_parts.extend([module] * 4)
    
    # ç‰¹å¾µæè¿°
    if pd.notna(row.get('features')):
        features = str(row['features'])
        text_parts.append(features)
        # æå–ç’°å¢ƒå’Œæ‡‰ç”¨é—œéµå­—
        keywords = extract_application_keywords(features)
        text_parts.extend(keywords)
    
    # ç’°å¢ƒé©ç”¨æ€§è³‡è¨Š
    env_info = extract_environmental_suitability(row)
    text_parts.extend(env_info)
    
    return " ".join(text_parts)

def extract_application_keywords(features_text):
    keywords = []

    application_patterns = {
        'å®¤å…§ç›£æ§': [r'å®¤å…§', r'å»ºç¯‰', r'è¾¦å…¬', r'æ©Ÿæˆ¿'],
        'å®‰å…¨ç›£æ§': [r'ç›£æ§', r'å®‰å…¨', r'é è­¦', r'è­¦å ±'],
        'ç†±æºåµæ¸¬': [r'ç†±æº', r'ç†±é¡¯åƒ', r'ç´…å¤–ç·š', r'æº«åº¦'],
        'äººå“¡åµæ¸¬': [r'äººå“¡', r'äººåƒ', r'äººæµ', r'é«”æº«'],
        'ç«ç½é é˜²': [r'ç«ç½', r'ç«æº', r'ç…™éœ§', r'é è­¦'],
        'ç’°å¢ƒç›£æ¸¬': [r'ç’°å¢ƒ', r'æ°£å€™', r'ç©ºæ°£', r'å“è³ª'],
        'å·¥æ¥­æ‡‰ç”¨': [r'å·¥å» ', r'å·¥æ¥­', r'æ©Ÿæ¢°', r'è¨­å‚™'],
        'è¾²æ¥­æ‡‰ç”¨': [r'è¾²æ¥­', r'æº«å®¤', r'åœŸå£¤', r'ç¨®æ¤'],
        'æˆ¶å¤–æ‡‰ç”¨': [r'æˆ¶å¤–', r'æ£®æ—', r'é‡å¤–', r'æ°£è±¡']
    }
    
    for app_type, patterns in application_patterns.items():
        if any(re.search(pattern, features_text) for pattern in patterns):
            keywords.extend([app_type] * 2)
    
    return keywords

def extract_environmental_suitability(row):
    env_info = []
    
    # IPç­‰ç´š
    if pd.notna(row.get('ip_rating')):
        ip_rating = str(row['ip_rating']).upper()
        if 'IP65' in ip_rating or 'IP66' in ip_rating:
            env_info.extend(['é˜²å¡µé˜²æ°´', 'å®¤å¤–é©ç”¨', 'æƒ¡åŠ£ç’°å¢ƒ'] * 2)
        elif 'IPX7' in ip_rating or 'IPX8' in ip_rating:
            env_info.extend(['é˜²æ°´', 'æˆ¶å¤–å¯ç”¨'])
        elif ip_rating != 'æœªæ¨™ç¤º' and ip_rating != 'æœªæŒ‡å®š':
            env_info.append('ç’°å¢ƒä¿è­·')
    
    # å·¥ä½œæº«åº¦
    if pd.notna(row.get('operating_temp')):
        temp_range = str(row['operating_temp'])
        if '-' in temp_range:
            try:
                temps = re.findall(r'-?\d+', temp_range)
                if len(temps) >= 2:
                    min_temp = int(temps[0])
                    max_temp = int(temps[1])
                    
                    if min_temp <= -20:
                        env_info.extend(['æ¥µä½æº«', 'åš´å¯’ç’°å¢ƒ'])
                    if max_temp >= 85:
                        env_info.extend(['é«˜æº«', 'å·¥æ¥­ç’°å¢ƒ'])
                    if min_temp >= 0 and max_temp <= 50:
                        env_info.extend(['å®¤å…§ç’°å¢ƒ', 'ä¸€èˆ¬ç’°å¢ƒ'])
                    else:
                        env_info.extend(['å¯¬æº«ç¯„åœ', 'æƒ¡åŠ£ç’°å¢ƒ'])
            except:
                pass
    
    # åŠŸè€—
    if pd.notna(row.get('power_consumption')):
        try:
            power = float(row['power_consumption'])
            if power <= 0.01:
                env_info.extend(['è¶…ä½åŠŸè€—', 'é›»æ± ä¾›é›»'])
            elif power <= 0.1:
                env_info.extend(['ä½åŠŸè€—', 'ç¯€èƒ½'])
        except:
            pass
    
    return env_info

def analyze_user_intent(user_input: str):
    """åˆ†æä½¿ç”¨è€…çš„å…·é«”éœ€æ±‚æ„åœ–ï¼Œå€åˆ†ç›´æ¥éœ€æ±‚å’Œç’°å¢ƒæè¿°"""
    user_lower = user_input.lower()
    comprehensive_analysis = {
        # intent
        'direct_sensor_needs': [],      # ç›´æ¥çš„æ„Ÿæ¸¬å™¨éœ€æ±‚
        'environmental_context': [],    # ç’°å¢ƒèƒŒæ™¯æè¿°
        'exclude_keywords': [],         # éœ€è¦æ’é™¤çš„é—œéµå­—
        
        # needs
        'primary_application': None,    # ä¸»è¦æ‡‰ç”¨å ´æ™¯
        'environment_needs': [],        # ç’°å¢ƒé©æ‡‰éœ€æ±‚
        'technical_specs': [],          # æŠ€è¡“è¦æ ¼éœ€æ±‚
        'priority_features': [],        # å„ªå…ˆåŠŸèƒ½ç‰¹æ€§

        'application_domain': None,     # æ‡‰ç”¨é ˜åŸŸ
        'deployment_context': [],       # éƒ¨ç½²ç’°å¢ƒ
        'performance_requirements': [] # æ•ˆèƒ½éœ€æ±‚
    }

    # ç›´æ¥æ„Ÿæ¸¬å™¨éœ€æ±‚è­˜åˆ¥
    direct_needs_patterns = {
        'ç†±é¡¯åƒ': [r'ç´…å¤–ç·š.*ç†±é¡¯åƒ', r'ç†±é¡¯åƒ.*æ¨¡çµ„', r'ç†±é¡¯åƒ', r'ç†±åƒå„€', r'é«”æº«.*æ„Ÿæ¸¬', r'æº«åº¦ç•°å¸¸', r'ç´…å¤–ç·š.*å½±åƒ'],
        'æ¯«ç±³æ³¢é›·é”': [r'æ¯«ç±³æ³¢.*äººæµ', r'æ¯«ç±³æ³¢.*æ¨¡çµ„', r'äººæµ.*è¨ˆç®—', r'äººå“¡.*è¿½è¹¤', r'å‹•ç·š.*ç›£æ§', r'äººé«”.*åµæ¸¬', r'æ¯«ç±³æ³¢é›·é”'],
        'æ°£é«”æ„Ÿæ¸¬': [r'æ°£é«”.*æ„Ÿæ¸¬', r'æ°£é«”.*åµæ¸¬', r'æ°£é«”.*æª¢æ¸¬', r'co2.*æ„Ÿæ¸¬', r'coâ‚‚.*æ„Ÿæ¸¬', r'voc.*æ„Ÿæ¸¬', r'ç©ºæ°£å“è³ª.*æ„Ÿæ¸¬', r'è‡­æ°§.*æ„Ÿæ¸¬', r'ä¸€æ°§åŒ–ç¢³.*æ„Ÿæ¸¬', r'æ°¨æ°£.*æ„Ÿæ¸¬', r'å¯ç‡ƒæ°£é«”.*æ„Ÿæ¸¬'],
        'äºŒæ°§åŒ–ç¢³æ°£é«”æ„Ÿæ¸¬': [r'äºŒæ°§åŒ–ç¢³.*æ„Ÿæ¸¬', r'co2.*æ„Ÿæ¸¬',r'co2', r'äºŒæ°§åŒ–ç¢³', r'coâ‚‚.*æ¿ƒåº¦', r'co2æ¿ƒåº¦', r'co2.*æª¢æ¸¬', r'coâ‚‚(æ„Ÿæ¸¬å™¨|æ¨¡çµ„|è¨­å‚™)'],
        'æº«æ¿•åº¦': [r'æº«æ¿•åº¦.*æ„Ÿæ¸¬', r'æº«åº¦.*æ¿•åº¦.*ç›£æ§',  r'æº«æ¿•åº¦',r'ç’°å¢ƒ.*æº«æ¿•åº¦', r'æº«åº¦.*æ„Ÿæ¸¬', r'æ¿•åº¦.*æ„Ÿæ¸¬', r'å®¤å…§.*æº«åº¦'],
        'ç’°å¢ƒå…‰æ„Ÿæ¸¬': [r'å…‰ç…§.*æ„Ÿæ¸¬', r'ç…§åº¦.*åµæ¸¬', r'äº®åº¦.*ç›£æ§', r'ç’°å¢ƒå…‰.*æ„Ÿæ¸¬'],
        'å‚¾æ–œ/æŒ¯å‹•': [r'å‚¾æ–œ.*åµæ¸¬', r'æŒ¯å‹•.*æ„Ÿæ¸¬', r'è§’åº¦.*æ„Ÿæ¸¬', r'æ™ƒå‹•.*æ„Ÿæ¸¬', r'åœ°éœ‡.*é è­¦', r'çµæ§‹.*ç›£æ§',r'å‚¾æ–œ.*(æ„Ÿæ¸¬å™¨|æ¨¡çµ„|è¨­å‚™)?'],
        'è¶…éŸ³æ³¢é¢¨é€Ÿé¢¨å‘': [r'é¢¨é€Ÿ.*æ„Ÿæ¸¬', r'é¢¨å‘.*æ„Ÿæ¸¬', r'é¢¨åŠ›.*åµæ¸¬', r'é¢¨é€Ÿé¢¨å‘', r'æ°£è±¡.*ç›£æ§', r'è¶…éŸ³æ³¢.*é¢¨é€Ÿ']
    }

    
    environmental_context_patterns = {
        'ä½æº«ç’°å¢ƒ': {
            'patterns': [r'ä½æº«.*ç’°å¢ƒ', r'å†·è—.*å€‰åº«', r'å†·å‡.*ç’°å¢ƒ', r'æ¥µä½æº«'],
            'requirement': 'ä½æº«ç’°å¢ƒé©ç”¨'
        },
        'é«˜æº«ç’°å¢ƒ': {
            'patterns': [r'é«˜æº«.*ç’°å¢ƒ', r'æ¥µé«˜æº«'],
            'requirement': 'é«˜æº«ç’°å¢ƒé©ç”¨'
        },
        'é«˜æ¿•ç’°å¢ƒ': {
            'patterns': [r'é«˜æ¿•.*ç’°å¢ƒ', r'æ½®æ¿•.*ç’°å¢ƒ', r'æ¿•åº¦.*è¼ƒé«˜'],
            'requirement': 'æŠ—æ¿•ç’°å¢ƒ'
        },
        'å·¥æ¥­ç’°å¢ƒ': {
            'patterns': [r'å·¥å» .*ç’°å¢ƒ', r'å·¥æ¥­.*ç¾å ´', r'ç”Ÿç”¢.*ç’°å¢ƒ'],
            'requirement': 'å·¥æ¥­ç´šè€ç”¨æ€§'
        },
        'å®¤å…§ç’°å¢ƒ': {
            'patterns': [r'å®¤å…§.*ç’°å¢ƒ', r'å»ºç¯‰.*å…§éƒ¨', r'å¯†é–‰.*ç©ºé–“'],
            'requirement': 'å®¤å…§éƒ¨ç½²é©ç”¨'
        },
        'æˆ¶å¤–ç’°å¢ƒ': {
            'patterns': [r'æˆ¶å¤–.*ç’°å¢ƒ', r'é‡å¤–.*æ‡‰ç”¨', r'å®¤å¤–.*ç›£æ§'],
            'requirement': 'æˆ¶å¤–é˜²è­·ç­‰ç´š'
        }
    }
    
    technical_specs_patterns = {
        'AIåŠŸèƒ½': {
            'patterns': [r'AI', r'äººå·¥æ™ºæ…§', r'æ©Ÿå™¨å­¸ç¿’', r'è¡Œç‚ºè¾¨è­˜', r'æ™ºèƒ½åˆ†æ'],
            'spec': 'AIæ“´å……ç›¸å®¹æ€§'
        },
        'å³æ™‚ç›£æ§': {
            'patterns': [r'å³æ™‚', r'å¯¦æ™‚', r'é€£çºŒç›£æ¸¬', r'æŒçºŒç›£æ§'],
            'spec': 'é€£çºŒç›£æ§èƒ½åŠ›'
        },
        'ç„¡ç·šå‚³è¼¸': {
            'patterns': [r'ç„¡ç·š', r'wifi', r'è—ç‰™', r'zigbee', r'lora'],
            'spec': 'ç„¡ç·šé€šè¨ŠåŠŸèƒ½'
        },
        'ä½åŠŸè€—': {
            'patterns': [r'ä½åŠŸè€—', r'çœé›»', r'é›»æ± ä¾›é›»', r'ç¯€èƒ½'],
            'spec': 'ä½åŠŸè€—è¨­è¨ˆ'
        },
        'é«˜ç²¾åº¦': {
            'patterns': [r'é«˜ç²¾åº¦', r'ç²¾ç¢º', r'æº–ç¢ºåº¦', r'ç²¾å¯†'],
            'spec': 'é«˜ç²¾åº¦æ¸¬é‡'
        }
    }
  # === æ‡‰ç”¨é ˜åŸŸè­˜åˆ¥ ===
    application_domain_patterns = {
        'æ™ºæ…§è¾²æ¥­': [r'è¾²æ¥­', r'æº«å®¤', r'ç¨®æ¤', r'è¾²ä½œç‰©', r'çŒæº‰'],
        'å·¥æ¥­ç›£æ§': [r'å·¥å» ', r'ç”Ÿç”¢ç·š', r'æ©Ÿæ¢°', r'è¨­å‚™ç›£æ§', r'é æ¸¬ç¶­è­·'],
        'å»ºç¯‰å®‰å…¨': [r'å»ºç¯‰', r'çµæ§‹', r'å®‰å…¨ç›£æ§', r'ç«ç½é è­¦', r'å®‰é˜²'],
        'ç’°å¢ƒç›£æ¸¬': [r'ç’°å¢ƒ', r'æ°£å€™', r'ç©ºæ°£å“è³ª', r'æ±¡æŸ“ç›£æ¸¬', r'æ°£è±¡'],
        'æ™ºæ…§åŸå¸‚': [r'åŸå¸‚', r'äº¤é€š', r'å…¬å…±è¨­æ–½', r'æ™ºæ…§è·¯ç‡ˆ', r'äººæµçµ±è¨ˆ']
    }

    
    # 1. ç›´æ¥æ„Ÿæ¸¬å™¨éœ€æ±‚
    for need_type, patterns in direct_needs_patterns.items():
        if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in patterns):
            comprehensive_analysis['direct_sensor_needs'].append(need_type)

    # 2. ç’°å¢ƒèƒŒæ™¯
    for env_type, config in environmental_context_patterns.items():
        if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in config['patterns']):
            comprehensive_analysis['environmental_context'].append(env_type)
            comprehensive_analysis['environment_needs'].append(config['requirement'])

    # 3. æŠ€è¡“è¦æ ¼éœ€æ±‚
    for tech_type, config in technical_specs_patterns.items():
        if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in config['patterns']):
            comprehensive_analysis['technical_specs'].append(config['spec'])

    # 4. æ‡‰ç”¨é ˜åŸŸ
    for domain, patterns in application_domain_patterns.items():
        if any(re.search(pattern, user_input, re.IGNORECASE) for pattern in patterns):
            comprehensive_analysis['application_domain'] = domain
            break
    # exclude_keywords
    if any(ctx in comprehensive_analysis['environmental_context'] for ctx in ['ä½æº«ç’°å¢ƒ', 'é«˜æ¿•ç’°å¢ƒ','é«˜æº«ç’°å¢ƒ']):
        if 'æº«æ¿•åº¦' not in comprehensive_analysis['direct_sensor_needs']:
            comprehensive_analysis['exclude_keywords'].append('æº«æ¿•åº¦')

    return comprehensive_analysis
    
# æ„Ÿæ¸¬å™¨é¡å‹åŒ¹é…åº¦è¨ˆç®—

def calculate_sensor_type_similarity(user_input: str, df: pd.DataFrame):
    intent = analyze_user_intent(user_input)
    similarities = []
    user_lower = user_input.lower()

    # æ„Ÿæ¸¬å™¨é¡å‹é—œéµå­—
    sensor_type_keywords = {
        'ç†±é¡¯åƒ': {
            'primary': ['ç†±é¡¯åƒ', 'ç´…å¤–ç·š', 'ç†±æº', 'ç«æº', 'é«”æº«', 'æº«åº¦å½±åƒ', 'äººå“¡è¿½è¹¤', 'ç†±æˆåƒ'],
            'secondary': ['ç„¡æ¥è§¸', 'ç›£æ§'],
            # 'exclude': ['å‚¾æ–œ', 'æŒ¯å‹•']
        },
        'æº«æ¿•åº¦': {
            'primary': ['æº«åº¦', 'æ¿•åº¦', 'æº«æ¿•åº¦', 'ç’°å¢ƒæº«åº¦', 'ç’°å¢ƒæ¿•åº¦','ç©ºæ°£æ¿•æ½¤åº¦'],
            'secondary': ['ç›£æ§', 'æ§åˆ¶', 'è¾²æ¥­', 'ç’°å¢ƒ', 'æº«å®¤', 'æ©Ÿæˆ¿', 'å€‰å„²']
        },

        'æ°£é«”æ„Ÿæ¸¬': {
            'primary': ['æ°¨æ°£', 'æ°£é«”', 'ç©ºæ°£å“è³ª', 'ä¸€æ°§åŒ–ç¢³', 'äºŒæ°§åŒ–ç¢³','co2', 'coâ‚‚', "ç”²çƒ·", "VOC", "æ°£é«”æª¢æ¸¬", "æ°£é«”æ´©æ¼",
                        'å¯ç‡ƒæ°£é«”','ç©ºæ°£å“è³ªç›£æ§', 'å®¤å…§ç©ºæ°£', 'é€šé¢¨','ç©ºæ°£ç›£æ¸¬', 'ç©ºæ°£æª¢æ¸¬','æ’é¢¨','æ°£é«”æ¿ƒåº¦'],
            'secondary': ['å·¥æ¥­ç›£æ§', 'æ°£é«”æ´©æ¼'],
        },
        'æ¯«ç±³æ³¢é›·é”': {
            'primary': ['æ¯«ç±³æ³¢', 'é›·é”', 'äººæµ', 'äººæ•¸çµ±è¨ˆ', 'å‹•ç·šè¿½è¹¤'],
            'secondary': ['äººå“¡åµæ¸¬', 'æµé‡çµ±è¨ˆ'],
        },
        'è¶…éŸ³æ³¢é¢¨é€Ÿé¢¨å‘': {
            'primary': ['é¢¨é€Ÿ', 'é¢¨å‘', 'æ°£è±¡', 'é¢¨åŠ›'],
            'secondary': ['æ°£è±¡', 'æ°£å€™'],
        },
        'ç’°å¢ƒå…‰æ„Ÿæ¸¬': {
            'primary': ['å…‰ç…§', 'ç…§åº¦', 'äº®åº¦', 'ç’°å¢ƒå…‰', 'å…‰åº¦','å…‰ç…§åº¦','æ—¥ç…§å¼·åº¦'],
            'secondary': ['è¾²æ¥­', 'æ—¥ç…§', 'å…‰æº'],
        },
        'å‚¾æ–œ/æŒ¯å‹•': {
            'primary': ['å‚¾æ–œ', 'æŒ¯å‹•', 'è§’åº¦', 'ç©©å®šæ€§ç›£æ¸¬', 'æ©Ÿæ¢°æŒ¯å‹•', 'çµæ§‹ç›£æ¸¬'],
            'secondary': ['æ©Ÿæ¢°ç›£æ¸¬', 'è¨­å‚™ç›£æ§'],
        },        
        'äºŒæ°§åŒ–ç¢³æ°£é«”æ„Ÿæ¸¬': {
            'primary': ['CO2', 'COâ‚‚', 'äºŒæ°§åŒ–ç¢³', 'ç©ºæ°£å“è³ª', 'ç©ºæ°£ç›£æ¸¬', 'co2', 'coâ‚‚',  'äºŒæ°§åŒ–ç¢³æ¿ƒåº¦', 
            'CO2æ¿ƒåº¦', "ppm",'ç«ç½'],
            'secondary': ['é€šé¢¨', 'ç©ºèª¿', 'å®¤å…§ç’°å¢ƒ'],
        },
        
    }

    for _, row in df.iterrows():
        max_similarity = 0.0
        sensor_type = str(row.get('type', '')).strip()

        # æ’é™¤æŒ‡å®šé—œéµå­—
        if any(ex in sensor_type for ex in intent['exclude_keywords']):
            similarities.append(0.0)
            continue

        # ç›´æ¥éœ€æ±‚çµ¦é«˜åˆ†
        if sensor_type in intent['direct_sensor_needs']:
            similarities.append(0.9)
            continue

        # æ ¹æ“šé—œéµå­—è¨ˆç®—åŒ¹é…åº¦
        if sensor_type in sensor_type_keywords:
            kws = sensor_type_keywords[sensor_type]
            primary_matches = sum(1 for kw in kws['primary'] if kw in user_lower)
            secondary_matches = sum(1 for kw in kws['secondary'] if kw in user_lower)
            if primary_matches:
                if len(kws['primary'])>12:
                    primary_score = min(primary_matches / (len(kws['primary'])/2.5) * 4, 1.0)
                    secondary_score = min(secondary_matches / (len(kws['secondary'])/1.3) * 0.3, 0.2)
                elif len(kws['primary'])>5:
                    primary_score = min(primary_matches / (len(kws['primary'])/1.7) * 4, 1.0)
                    secondary_score = min(secondary_matches / (len(kws['secondary'])/1.3) * 0.3, 0.2)
                else:
                    primary_score = min(primary_matches / (len(kws['primary'])/1.4) * 4, 1.0)
                    secondary_score = min(secondary_matches / (len(kws['secondary'])/1) * 0.3, 0.2)
                max_similarity = min(primary_score + secondary_score, 1.0)

        similarities.append(max_similarity)

    return np.array(similarities)

#æ¨è–¦é‚è¼¯
def recommend_advanced(user_input: str, df, model, device_embeddings, 
                        sensor_type_weight: float = 0.4,    # é¡å‹æ¬Šé‡
                        module_weight: float = 0.3,         # æ¨¡çµ„æ¬Šé‡
                        semantic_weight: float = 0.25,       # èªæ„æ¬Šé‡
                        environment_weight: float = 0.05,    # ç’°å¢ƒæ¬Šé‡
                        threshold:float = 0.5,            
                        top_k:int= 3):
    
    if df is None or model is None or device_embeddings is None:
        return None
    
    try:
        # 1. èªæ„ç›¸ä¼¼åº¦è¨ˆç®—
        user_embedding = model.encode(user_input, convert_to_tensor=True)
        semantic_similarities = util.cos_sim(user_embedding, device_embeddings)[0].cpu().numpy()
        
        # 2. æ„Ÿæ¸¬å™¨é¡å‹åŒ¹é…åº¦
        sensor_type_similarities = calculate_sensor_type_similarity(user_input, df)
        
        # 3. æ¨¡çµ„åŒ¹é…åº¦
        module_similarities = calculate_module_similarity(user_input, df)
        
        # 4. ç’°å¢ƒé©ç”¨æ€§åŒ¹é…åº¦
        environment_similarities = calculate_environment_similarity(user_input, df)
        
        # 5. ç¶œåˆè©•åˆ†
        final_scores = (sensor_type_similarities * sensor_type_weight + 
                       module_similarities * module_weight + 
                       semantic_similarities * semantic_weight +
                       environment_similarities * environment_weight)
        
        # 6. å»ºç«‹çµæœ
        df_copy = df.copy()
        df_copy["sensor_type_similarity"] = sensor_type_similarities
        df_copy["module_similarity"] = module_similarities
        df_copy["semantic_similarity"] = semantic_similarities
        df_copy["environment_similarity"] = environment_similarities
        df_copy["final_score"] = final_scores
        
        # 7. ç¯©é¸å’Œæ’åº
        matched = df_copy[df_copy["final_score"] >= threshold].sort_values(
            by="final_score", ascending=False)
        
        if matched.empty:
            return None
        
        # 8. é¸æ“‡é¡¯ç¤ºæ¬„ä½
        display_columns = ['name', 'type', 'final_score', 'sensor_type_similarity',
                          'module_similarity', 'semantic_similarity', 'environment_similarity',
                          'parsed_modules', 'features', 'ip_rating', 'power_consumption', 
                          'operating_temp', 'range', 'precision']
        
        # åªä¿ç•™å­˜åœ¨çš„æ¬„ä½
        display_columns = [col for col in display_columns if col in matched.columns]
        
        result = matched[display_columns].head(top_k).reset_index(drop=True)
        
        # æ ¼å¼åŒ–åˆ†æ•¸
        score_columns = ['final_score', 'sensor_type_similarity', 'module_similarity', 
                        'semantic_similarity', 'environment_similarity']
        for score_col in score_columns:
            if score_col in result.columns:
                result[score_col] = result[score_col].round(3)
        
        return result
        
    except Exception as e:
        print(f"æ¨è–¦éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return None

def calculate_module_similarity(user_input: str, df):
    similarities = []
    user_lower = user_input.lower()

    application_keywords = {
        'æº«æ¿•åº¦ç›£æ§åµæ¸¬': {
            'primary': ['æº«åº¦', 'æ¿•åº¦', 'æº«æ¿•åº¦', 'ç’°å¢ƒæº«åº¦', 'ç’°å¢ƒæ¿•åº¦'],
            'secondary': ['è¾²æ¥­', 'æ°£å€™', 'ç’°å¢ƒç›£æ§'],
            'context': ['æŒçºŒç›£æ¸¬', 'è¨˜éŒ„', 'æ„Ÿæ¸¬'],
        },
        'ç«ç½é è­¦åµæ¸¬': {
            'primary': ['ç«ç½', 'ç«æº', 'ç†±æº', 'å®‰å…¨ç›£æ§', 'é è­¦', 'ç†±é¡¯åƒ', 'co2', 'COâ‚‚','äºŒæ°§åŒ–ç¢³','æ¿ƒåº¦'],
            'secondary': ['å»ºç¯‰å®‰å…¨', 'ç›£æ§ç³»çµ±', 'ç´…å¤–ç·š', 'æº«åº¦ç›£æ¸¬'],
            'context': ['å³æ™‚', 'è‡ªå‹•', 'è­¦å ±', 'åµæ¸¬'],
        },
        'ç«ç½é è­¦åµæ¸¬(wifi)': {
            'primary': ['ç«ç½', 'ç„¡ç·š', 'wifi', 'é ç«¯ç›£æ§', 'ç†±é¡¯åƒ', 'co2', 'COâ‚‚' , 'äºŒæ°§åŒ–ç¢³','æ¿ƒåº¦'],
            'secondary': ['ç‰©è¯ç¶²', 'iot', 'é›²ç«¯', 'ç„¡ç·šå‚³è¼¸'],
            'context': ['å³æ™‚å‚³è¼¸', 'é ç«¯', 'ç„¡ç·šé€šè¨Š'],
        },
        'é«”æº«åµæ¸¬': {
            'primary': ['é«”æº«', 'ç´…å¤–ç·šæ„Ÿæ¸¬', 'ç†±é¡¯åƒ', 'é¡æº«åµæ¸¬'],
            'secondary': ['ç™¼ç‡’', 'é«”è¡¨æº«åº¦', 'é†«ç™‚åµæ¸¬', 'å¥åº·ç›£æ§'],
            'context': ['å³æ™‚å‚³è¼¸', 'é ç«¯', 'ä¸æ¥è§¸'],
        },
        'æ°£å€™å…‰ç…§åµæ¸¬': {
            'primary': ['å…‰ç…§', 'ç…§åº¦', 'ç’°å¢ƒå…‰', 'æ°£å€™', 'co2', 'COâ‚‚','äºŒæ°§åŒ–ç¢³'],
            'secondary': ['è¾²æ¥­', 'æ¤ç‰©ç”Ÿé•·', 'æº«å®¤', 'ç’°å¢ƒç›£æ¸¬'],
            'context': ['å…‰ç·šæ„Ÿæ¸¬', 'æ°£å€™ç›£æ§', 'ç’°å¢ƒåƒæ•¸'],
        },
        'åœŸå£¤æ°£å€™æ•´åˆåµæ¸¬': {
            'primary': ['åœŸå£¤', 'æ°£å€™', 'è¾²æ¥­', 'ç¨®æ¤', 'co2','COâ‚‚', 'å…‰ç…§', 'æº«æ¿•åº¦'],
            'secondary': ['æ™ºæ…§è¾²æ¥­', 'ç²¾æº–è¾²æ¥­', 'æ¤ç‰©ç”Ÿé•·'],
            'context': ['æ•´åˆç›£æ¸¬', 'å¤šåƒæ•¸', 'è¾²æ¥­æ‡‰ç”¨'],
        },
        'ç„¡CO2åœŸå£¤æ°£å€™åµæ¸¬': {
            'primary': ['åœŸå£¤', 'æ°£å€™', 'å…‰ç…§', 'ç’°å¢ƒç›£æ¸¬', 'ç„¡co2'],
            'secondary': ['ç°¡åŒ–ç›£æ¸¬', 'åŸºç¤è¾²æ¥­', 'ç’°å¢ƒæ„Ÿæ¸¬'],
            'context': ['åŸºæœ¬åƒæ•¸', 'æˆæœ¬å„ªåŒ–'],
        },
        'äººæµè¨ˆç®—èˆ‡æ°¨æ°£æ„Ÿæ¸¬': {
            'primary': ['äººæµ', 'äººæ•¸çµ±è¨ˆ', 'æ°¨æ°£', 'æ¯«ç±³æ³¢', 'é›·é”', 'ç©ºæ°£å“è³ª', 'ç©ºæ°£ç›£æ§','ç©ºæ°£ç•°å‘³','æœ‰å®³æ°£é«”', 'ç©ºæ°£','äººæ½®','æ’é¢¨','æ›æ°£'],
            'secondary': ['ç©ºæ°£å“è³ª', 'äººå“¡çµ±è¨ˆ','é€šé¢¨','æ’é¢¨ç³»çµ±','é™¤è‡­','ç©ºé–“ä½¿ç”¨'],
            'context': ['é›·é”åµæ¸¬', 'æ°£é«”ç›£æ¸¬', 'é›™é‡åŠŸèƒ½'],
        },
        'æ£®æ—æ‡‰ç”¨ç›£æ¸¬': {
            'primary': ['æ£®æ—', 'é¢¨åŠ›', 'é¢¨å‘', 'é¢¨é€Ÿ', 'æˆ¶å¤–ç›£æ¸¬'],
            'secondary': ['æ°£è±¡', 'ç’°å¢ƒç›£æ¸¬', 'é‡å¤–æ‡‰ç”¨'],
            'context': ['è¶…éŸ³æ³¢', 'æ°£è±¡åƒæ•¸', 'æˆ¶å¤–ç’°å¢ƒ'],
        },
        'å‚¾æ–œåµæ¸¬': {
            'primary': ['å‚¾æ–œ', 'è§’åº¦', 'ç©©å®šæ€§', 'çµæ§‹ç›£æ¸¬', 'æŒ¯å‹•'],
            'secondary': ['å»ºç¯‰å®‰å…¨', 'çµæ§‹å¥åº·', 'è¨­å‚™ç›£æ§'],
            'context': ['é›™è»¸', 'ç²¾å¯†æ¸¬é‡', 'å®‰å…¨ç›£æ§'],
        },
        'é¦¬é”åµæ¸¬': {
            'primary': ['é¦¬é”', 'æŒ¯å‹•', 'è¨­å‚™ç›£æ§', 'æ©Ÿæ¢°', 'å‚¾æ–œ'],
            'secondary': ['å·¥æ¥­è¨­å‚™', 'é æ¸¬ç¶­è­·', 'æ©Ÿæ¢°å¥åº·'],
            'context': ['è¨­å‚™è¨ºæ–·', 'æŒ¯å‹•åˆ†æ', 'é é˜²ä¿é¤Š'],
        }
    }

    for _, row in df.iterrows():
        max_similarity = 0.0

        if 'parsed_modules' in row.index and row['parsed_modules']:
            for module in row['parsed_modules']:
                module_clean = re.sub(r'[ï¼ˆ(].*?[ï¼‰)]', '', module.lower())
                module_clean = module_clean.replace("åµæ¸¬å™¨", "åµæ¸¬")

                if module_clean in user_lower:
                    max_similarity = max(max_similarity, 1.0)
                    continue

                matched_key = None
                for key in application_keywords:
                    if module_clean in key.lower() or key.lower() in module_clean:
                        matched_key = key
                        break

                if not matched_key:
                    matched_keys = get_close_matches(module_clean, [k.lower() for k in application_keywords.keys()], n=1, cutoff=0.6)
                    if matched_keys:
                        matched_key = next((k for k in application_keywords if k.lower() == matched_keys[0]), None)

                if matched_key:
                    keywords = application_keywords[matched_key]
                    primary = sum(1 for kw in keywords['primary'] if kw in user_lower)
                    secondary = sum(1 for kw in keywords['secondary'] if kw in user_lower)
                    context = sum(1 for kw in keywords['context'] if kw in user_lower)

                    weight = (primary * 1 + secondary * 0.2 + context * 0.05)
                    total = (len(keywords['primary']) * 0.4 + len(keywords['secondary']) * 0.07 + len(keywords['context']) * 0.02)
                    similarity = weight / total
                    max_similarity = max(max_similarity, similarity)

        similarities.append(min(max_similarity,1))

    return np.array(similarities)

def calculate_environment_similarity(user_input: str, df):
    similarities = []
    user_lower = user_input.lower()
    
    # ç’°å¢ƒéœ€æ±‚é—œéµå­—
    env_requirements = {
        'ä½æº«ç’°å¢ƒ': ['ä½æº«', 'å†·è—', 'å†·å‡', 'æ¥µä½æº«'],
        'é«˜æ¿•ç’°å¢ƒ': ['é«˜æ¿•', 'æ½®æ¿•', 'æŠ—æ¿•'],
        'AIæ“´å……': ['AI', 'äººå·¥æ™ºæ…§', 'æ©Ÿå™¨å­¸ç¿’', 'è¡Œç‚ºè¾¨è­˜'],
        'å³æ™‚ç›£æ§': ['å³æ™‚', 'å¯¦æ™‚', 'é€£çºŒç›£æ¸¬'],
        'äººå“¡è¿½è¹¤': ['äººå“¡', 'ç§»å‹•è»Œè·¡', 'è¿½è¹¤', 'è¡Œç‚º']
    }
    
    for _, row in df.iterrows():
        env_score = 0.0
        
        # ä½æº«ç’°å¢ƒé©ç”¨æ€§
        if any(keyword in user_lower for keyword in env_requirements['ä½æº«ç’°å¢ƒ']):
            if pd.notna(row.get('operating_temp')):
                temp_range = str(row['operating_temp'])
                if '-' in temp_range:
                    try:
                        temps = re.findall(r'-?\d+', temp_range)
                        if len(temps) >= 2:
                            min_temp = int(temps[0])
                            if min_temp <= -20:  # æ”¯æ´ä½æº«
                                env_score += 0.3
                    except:
                        pass
        
        # é«˜æ¿•ç’°å¢ƒæŠ—æ€§
        if any(keyword in user_lower for keyword in env_requirements['é«˜æ¿•ç’°å¢ƒ']):
            if pd.notna(row.get('ip_rating')):
                ip_rating = str(row['ip_rating']).upper()
                if any(rating in ip_rating for rating in ['IP65', 'IP66', 'IPX7']):
                    env_score += 0.2
        
        # AIæ“´å……èƒ½åŠ›
        if any(keyword in user_lower for keyword in env_requirements['AIæ“´å……']):
            if pd.notna(row.get('features')):
                features = str(row['features']).lower()
                if any(ai_feature in features for ai_feature in ['åˆ†æ', 'æ™ºèƒ½', 'æ“´å……', 'å¹³å°']):
                    env_score += 0.2
        
        # å³æ™‚ç›£æ§èƒ½åŠ›
        if any(keyword in user_lower for keyword in env_requirements['å³æ™‚ç›£æ§']):
            if pd.notna(row.get('features')):
                features = str(row['features']).lower()
                if any(realtime_feature in features for realtime_feature in ['å³æ™‚', 'é€£çºŒ', 'æŒçºŒ']):
                    env_score += 0.15
        
        similarities.append(min(env_score, 1.0))
    
    return np.array(similarities)


def interactive_recommend():
    print("=== æ„Ÿæ¸¬å™¨æ™ºæ…§æ¨è–¦ç³»çµ± v2.0 ===")
    
    # åˆå§‹åŒ–ç³»çµ±
    df, model, device_embeddings = initialize_system()
    
    if df is None:
        print("ç³»çµ±åˆå§‹åŒ–å¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™")
        return
    
    print(f"å·²è¼‰å…¥ {len(df)} æ¬¾æ„Ÿæ¸¬å™¨\n")
    
    while True:
        try:
            user_input = input("ğŸ” è«‹æè¿°æ‚¨çš„æ„Ÿæ¸¬å™¨æ‡‰ç”¨éœ€æ±‚ï¼š\n> ").strip()
            
            if user_input.lower() in ['q', 'quit', 'é›¢é–‹']:
                print("æ„Ÿè¬ä½¿ç”¨ï¼")
                break
            
            if not user_input:
                continue
            
            # æ„åœ–åˆ†æ
            intent = analyze_user_intent(user_input)
            print(f"\nğŸ§  éœ€æ±‚æ„åœ–åˆ†æï¼š")
            if intent['direct_sensor_needs']:
                print(f"   æ„Ÿæ¸¬å™¨éœ€æ±‚: {', '.join(intent['direct_sensor_needs'])}")
            #     weight_type=1
            # else:
            #     weight_type=2
            if intent['environmental_context']:
                print(f"   ç’°å¢ƒèƒŒæ™¯æè¿°: {', '.join(intent['environmental_context'])}")
            detailed_analysis = analyze_user_intent(user_input)
            if detailed_analysis['primary_application']:
                print(f"   ä¸»è¦æ‡‰ç”¨å ´æ™¯: {detailed_analysis['primary_application']}")
            
            print("\nğŸ”„ æ­£åœ¨åˆ†æä¸¦åŒ¹é…æ„Ÿæ¸¬å™¨...")
            
            result = recommend_advanced(
                    user_input, df, model, device_embeddings,
                    sensor_type_weight=0.4,
                    module_weight=0.3,
                    semantic_weight=0.25,
                    environment_weight=0.05,
                    threshold=0.5,
                    top_k=5
                )
            
            
            if result is None or result.empty:
                print("âŒ å¾ˆæŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç¬¦åˆéœ€æ±‚çš„æ„Ÿæ¸¬å™¨ã€‚")
                print("ğŸ”„ å˜—è©¦é™ä½é–€æª»é‡æ–°æœå°‹...")
                
                relaxed_result = recommend_advanced(
                    user_input, df, model, device_embeddings, 
                    threshold=0.4, top_k=3
                )
                
                if relaxed_result is not None and len(relaxed_result) > 0:
                    print(f"æ‰¾åˆ° {len(relaxed_result)} æ¬¾å¯èƒ½ç›¸é—œçš„æ„Ÿæ¸¬å™¨ï¼š")
                    for idx, row in relaxed_result.iterrows():
                        print(f"  â€¢ {row['name']} (ç¸½è©•åˆ†: {row['final_score']:.3f})")
                        print(f"   è©³ç´°è©•åˆ†:")
                        print(f"      â€¢ é¡å‹åŒ¹é…: {row.get('sensor_type_similarity', 0):.3f}")
                        print(f"      â€¢ æ¨¡çµ„ç›¸å®¹: {row.get('module_similarity', 0):.3f}")
                        print(f"      â€¢ ç’°å¢ƒé©ç”¨: {row.get('environment_similarity', 0):.3f}")
                        print(f"      â€¢ èªæ„ç›¸ä¼¼: {row.get('semantic_similarity', 0):.3f}")
                else:
                    print("âŒ å¾ˆæŠ±æ­‰ï¼Œæ²’æœ‰æ‰¾åˆ°ç¬¦åˆéœ€æ±‚çš„æ„Ÿæ¸¬å™¨ã€‚")

                print("ğŸ’¡ å»ºè­°ï¼š")
                print("   - è«‹å˜—è©¦ä½¿ç”¨æ›´å…·é«”çš„é—œéµå­—")
                print("   - æè¿°å…·é«”çš„æ‡‰ç”¨å ´æ™¯")
                print("   - èªªæ˜ç’°å¢ƒæ¢ä»¶éœ€æ±‚")
                continue
            
            # é¡¯ç¤ºæ¨è–¦çµæœ
            print(f"\nğŸ“Š æ‰¾åˆ° {len(result)} æ¬¾æ¨è–¦æ„Ÿæ¸¬å™¨ï¼š")
            print("=" * 80)
            
            for idx, row in result.iterrows():
                print(f"\nã€æ¨è–¦ {idx+1} ã€‘{row['name']}( {row['type']})")
                print(f"â­ ç¶œåˆè©•åˆ†: {row['final_score']:.3f}")
                
                # è©³ç´°è©•åˆ†åˆ†æ
                print(f"ğŸ“Š è©³ç´°è©•åˆ†:")
                print(f"   - é¡å‹åŒ¹é…åº¦: {row['sensor_type_similarity']:.3f}")
                print(f"   - æ¨¡çµ„åŒ¹é…åº¦: {row['module_similarity']:.3f}")
                print(f"   - èªæ„ç›¸ä¼¼åº¦: {row['semantic_similarity']:.3f}")
                print(f"   - ç’°å¢ƒé©ç”¨åº¦: {row['environment_similarity']:.3f}")
                
                # ç›¸å®¹æ¨¡çµ„
                if row['parsed_modules']:
                    modules_str = ', '.join(row['parsed_modules'])
                    print(f"æ¨¡çµ„: {modules_str}")
                
                if pd.notna(row.get('features')):
                    print(f"âœ¨ ä¸»è¦ç‰¹è‰²: {row['features']}")
                
                if 'ip_rating' in row.index and pd.notna(row['ip_rating']):
                        ip_rating = row['ip_rating']
                        if ip_rating not in ['æœªæ¨™ç¤º', 'æœªæŒ‡å®š']:
                            print(f"   ğŸ›¡ï¸ é˜²è­·ç­‰ç´š: {ip_rating}")
                            # IPç­‰ç´šèªªæ˜
                            if 'IP65' in str(ip_rating):
                                print(f"      â””â”€ å®Œå…¨é˜²å¡µï¼Œå¯é˜²å™´æ°´ (é©åˆå®¤å…§å¤–)")
                            elif 'IPX7' in str(ip_rating):
                                print(f"      â””â”€ å¯çŸ­æ™‚é–“æµ¸æ°´ (é©åˆæ½®æ¿•ç’°å¢ƒ)")
                # å·¥ä½œæº«åº¦
                if 'operating_temp' in row.index and pd.notna(row['operating_temp']):
                    print(f"   ğŸŒ¡ï¸ å·¥ä½œæº«åº¦: {row['operating_temp']}")
                    
                # åŠŸè€—
                if 'power_consumption' in row.index and pd.notna(row['power_consumption']):
                    power = row['power_consumption']
                    print(f"   ğŸ”‹ åŠŸè€—: {power}W", end="")
                    if float(power) <= 0.1:
                        print(" (ä½åŠŸè€—ï¼Œé©åˆé•·æœŸç›£æ§)")
                    else:
                        print()
                # ç²¾åº¦å’Œç¯„åœ
                if 'range' in row.index and pd.notna(row['range']):
                    range_info = str(row['range'])[:100] + "..." if len(str(row['range'])) > 100 else str(row['range'])
                    print(f"   ğŸ“ é‡æ¸¬ç¯„åœ: {range_info}")
                    
                if 'precision' in row.index and pd.notna(row['precision']):
                    precision = str(row['precision'])[:100] + "..." if len(str(row['precision'])) > 100 else str(row['precision'])
                    print(f"   ğŸ¯ ç²¾åº¦: {precision}")
                    
                 # æ¨è–¦èªªæ˜
                if row['module_similarity'] > 0.5 or row['environment_similarity'] > 0.2:
                    print("-" * 60)
                    print(f"\nğŸ’¡ æ¨è–¦èªªæ˜:")
                    row = result.iloc[0]
                    
                    if row['module_similarity'] > 0.5:
                        print(f"âœ… è©²æ„Ÿæ¸¬å™¨çš„ç›¸å®¹æ¨¡çµ„é©åˆæ‚¨çš„æ‡‰ç”¨å ´æ™¯")
                    
                    if row['environment_similarity'] > 0.2:
                        print(f"âœ… è©²æ„Ÿæ¸¬å™¨èƒ½é©æ‡‰æ‚¨æè¿°çš„ç’°å¢ƒæ¢ä»¶")
                
            print("\n" + "=" * 80)
            
        except KeyboardInterrupt:
            print("\n\nç¨‹å¼å·²ä¸­æ–·")
            break
        except Exception as e:
            print(f"âŒ æ¨è–¦éç¨‹ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            continue
class RecommendationRequest(BaseModel):
    query: str = Field(..., min_length=1, max_length=500)
    sensor_type_weight: float = Field(0.4, ge=0.0, le=1.0)
    module_weight: float = Field(0.3, ge=0.0, le=1.0)
    semantic_weight: float = Field(0.2, ge=0.0, le=1.0)
    environment_weight: float = Field(0.1, ge=0.0, le=1.0)
    threshold: float = Field(0.5, ge=0.0, le=1.0)
    top_k: int = Field(5, ge=1, le=20)


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    try:
        interactive_recommend()
    except Exception as e:
        print(f"ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼š{e}")

if __name__ == "__main__":
    main()